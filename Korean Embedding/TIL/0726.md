1. 벡터공간(Vector space)
벡터라는 개체를 원소로 가지는 공집합이 아닌 집합으로, 10개의 공리를 조건으로 합과 스칼라배(실수배)라는 두 연산이 정의된 집합이다. 즉, 벡터공간 내 모든 벡터는 어떤 실수 c,d에 대해 모든 공리를 만족해야한다.
2. 벡터의 내적(inner product)과 코사인 유사도(cosine similarilty)
- 벡터의 내적 : 두 벡터 a, b의 크기와 cos seta(a,b사이각도) 의 곱 , 즉 각도 seta에 따라 값이 음수나 양수가 될 수 있다.
- 코사인 유사도 : 두 벡터 사이의 코사인 값, 즉 두개가 같은 방향이면 1, 반대방향이면 -1 값을 나타낸다. 그러므로 값이 1에 가까울수록 유사도가 높다고 할 수 있다.(-1 ≤ cos ≤ 1) 
3. 고유 분해(eigen decomposition) 
- 고유값과 고유벡터를 찾는 작업 
4. 엔트로피, 크로스 엔트로피
- 엔트로피는 불확실성을 나타낸다. 즉, 어떤 데이터가 나올지 예측하기 어려운 경우 엔트로피값은 커진다. 예를 들어 주사위와 동전이 있다면 주사위값의 엔트로피가 더 높다. 직관적으로 엔트로피값이 높다는 것은 정보가 많고 확률이 낮다고 이해할 수 있다.
- 크로스 엔트로피는 실제값(q)을 알지 못할 때, 예측값(p)을 통해 q를 예측하는 것이다. 값의 크기는 q와 p가 동일할 때 0으로 수렴하고 틀릴 경우 값이 커지므로, q와 p의 차이를 줄이기 위한 엔트로피다.
5. 그래디언트 디센트
- 경사하강법, 함수의 기울기를 구하고 기울기(경사)의 절댓값이 낮은쪽으로 계속해서 이동시켜 극값에 이를때까지 반복시키는 방법이다. 
6. 사전확률(prior probability)과 사후확률(posteriori probability)
- 사전확률은 확률 시행 전 이미 가진 지식을 통해 부여한 확률
- 사후확률은 사건 발생 후에 어떤 원인으로부터 일어난 것이라고 생각되어지는 확률  e.g) 동전을 3번 던졌을 때 앞면이 나오지 않아서 동전 던지기전 사전확률(1/2) 보다 사후확률은 더 낮을 것이다.
7. 피드포워드 뉴럴 네트워크(feedforward neural network)
- 노드간 연결이 순환을 형성하지 않는 구조로, 최초의 인공신경망이며 가장 단순한 형태이다. 
- 입력층-은닉층-출력층 구조이며, 가중치의 반복적인 업데이트를 통해 출력값의 에러를 최소화하는 방향으로 학습한다.
8. CNN(Convolutional Neural Network) 
- 합성곱신경망, 합성곱 필터를 통해 특징을 추출해내고 풀링을 통해 추출한 이미지의 특징을 강화하는 형태의 인공신경망.
- 이미지의 공간 정보를 유지한 체 학습할 수 있다. 
9. RNN(Recurrent Neural Network) 
- 순환신경망, 맨앞에서 학습한 노드의 정보가 맨뒤까지 전달된다. (단, 학습이 느리다.)
