<h3>1.1 임베딩이란?</h3>
- 임베딩이란 사람이 사용하는 자연어(natural language)를 기계가 이해할 수 있는 벡터(vector)로 바꾼 결과 혹은 그 일련의 과정 전체를 의미한다. 벡터공간(vector space) 에 끼워넣는다(embed)는 의미에서 유래됐다.

<h3>1.2 임베딩의 역할 </h3>
→ 단어/문장 간 관련도 계산 , 의미적&문법적 정보 함축 , 전이 학습

1.2.1 단어&문장간 관련도 계산
- 말뭉치를 mecab을 통해 형태소분석 →100차원  word2vec 임베딩해준다. 그러면 이제 단어는 벡터가 되고, 벡터로 표현된 단어들 간에 유사도를 계산해줄 수 있다. 이 유사도를 통해 비슷한 단어들을 유추해볼 수 있고, 차원축소를 통해 2차원으로 시각화 해볼 수도 있다.

1.2.2 의미/문법 정보 함축
- 임베딩은 벡터로 표현되는 만큼 사칙연산이 가능하다. 즉 벡터1 - 벡터2 + 벡터3 이 가능하다. 만약 두 벡터 사이(아들 - 딸)의 의미 차이가 임베딩에 함축되어있으면 좋은 임베딩이라 말할 수 있다.이렇게 단어 임베딩 평가하는 방법을 "단어 유추 평가 word analogy test"라고 부른다.

1.2.3 전이학습
- 임베딩을 다른 딥러닝 모델의 입력값으로 쓰는 기법
- 전이 학습은 사람의 학습과 비슷하다. 사람이 무언갈 배울 때는 지금까지 내가 아는 모든 지식을 동원해 배운다. 그렇기에 빨리 배울 수 있는 것이다. 전이 학습도 이 원리와 동일하다. 기존에 학습한 임베딩을 사용해서 테스크를 빠르게 잘 할 수 있게한다.


<h3>1.3 임베딩 기법의 역사와 종류</h3>
->  임베딩 기법의 발전 흐름과 종류

1.3.1 통계 기반에서 뉴럴 네트워크 기반으로
- 통계 기반의 대표적인 기법인 LSA(Latent semantic analysis , 잠재 의미 분석) 은 전체 행렬(matrix) 에 특이값 분해(Singular Value Decomposition) 를 통해 차원 축소하는 방법이다. 그런데 전체 단어-문서 행렬은 너무 sparse하므로 계산량이 낭비된다. 고로 차원축소하여 사용한다. 
- 뉴럴 네트워크 기반 방법은 구조가 유연하고 표현력이 풍부하기 때문에 무한한 문맥을 상당 부분 학습할 수 있다. 문장 중간에 구멍을 뚫어놓고(masking) 해당 단어가 무엇일 지 맞추는 과정에서 학습된다.

### 사전 지식 정리 

1. 벡터공간(Vector space)
벡터라는 개체를 원소로 가지는 공집합이 아닌 집합으로, 10개의 공리를 조건으로 합과 스칼라배(실수배)라는 두 연산이 정의된 집합이다. 즉, 벡터공간 내 모든 벡터는 어떤 실수 c,d에 대해 모든 공리를 만족해야한다.
2. 벡터의 내적(inner product)과 코사인 유사도(cosine similarilty)
- 벡터의 내적 : 두 벡터 a, b의 크기와 cos seta(a,b사이각도) 의 곱 , 즉 각도 seta에 따라 값이 음수나 양수가 될 수 있다.
- 코사인 유사도 : 두 벡터 사이의 코사인 값, 즉 두개가 같은 방향이면 1, 반대방향이면 -1 값을 나타낸다. 그러므로 값이 1에 가까울수록 유사도가 높다고 할 수 있다.(-1 ≤ cos ≤ 1) 
3. 고유 분해(eigen decomposition) 
- 고유값과 고유벡터를 찾는 작업 
4. 엔트로피, 크로스 엔트로피
- 엔트로피는 불확실성을 나타낸다. 즉, 어떤 데이터가 나올지 예측하기 어려운 경우 엔트로피값은 커진다. 예를 들어 주사위와 동전이 있다면 주사위값의 엔트로피가 더 높다. 직관적으로 엔트로피값이 높다는 것은 정보가 많고 확률이 낮다고 이해할 수 있다.
- 크로스 엔트로피는 실제값(q)을 알지 못할 때, 예측값(p)을 통해 q를 예측하는 것이다. 값의 크기는 q와 p가 동일할 때 0으로 수렴하고 틀릴 경우 값이 커지므로, q와 p의 차이를 줄이기 위한 엔트로피다.
5. 그래디언트 디센트
- 경사하강법, 함수의 기울기를 구하고 기울기(경사)의 절댓값이 낮은쪽으로 계속해서 이동시켜 극값에 이를때까지 반복시키는 방법이다. 
6. 사전확률(prior probability)과 사후확률(posteriori probability)
- 사전확률은 확률 시행 전 이미 가진 지식을 통해 부여한 확률
- 사후확률은 사건 발생 후에 어떤 원인으로부터 일어난 것이라고 생각되어지는 확률  e.g) 동전을 3번 던졌을 때 앞면이 나오지 않아서 동전 던지기전 사전확률(1/2) 보다 사후확률은 더 낮을 것이다.
7. 피드포워드 뉴럴 네트워크(feedforward neural network)
- 노드간 연결이 순환을 형성하지 않는 구조로, 최초의 인공신경망이며 가장 단순한 형태이다. 
- 입력층-은닉층-출력층 구조이며, 가중치의 반복적인 업데이트를 통해 출력값의 에러를 최소화하는 방향으로 학습한다.
8. CNN(Convolutional Neural Network) 
- 합성곱신경망, 합성곱 필터를 통해 특징을 추출해내고 풀링을 통해 추출한 이미지의 특징을 강화하는 형태의 인공신경망.
- 이미지의 공간 정보를 유지한 체 학습할 수 있다. 
9. RNN(Recurrent Neural Network) 
- 순환신경망, 맨앞에서 학습한 노드의 정보가 맨뒤까지 전달된다. (단, 학습이 느리다.)
